{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "naughty-dancing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import TreebankWordTokenizer, WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-robertson",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "surprising-treat",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('data/alice.txt', 'r')\n",
    "text = str(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "religious-boating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alice’s Adventures in Wonderland\\n\\nby Lewis Carroll\\n\\nTHE MILLENNIUM FULCRUM EDITION 3.0\\n\\nContents\\n\\n CHAPTER I.     Down the Rabbit-Hole\\n CHAPTER II.    The Pool of Tears\\n CHAPTER III.   A Caucus-Race and a Long Tale\\n CHAPTER IV.    The Rabbit Sends in a Little Bill\\n CHAPTER V.     Advice from a Caterpillar\\n CHAPTER VI.    Pig and Pepper\\n CHAPTER VII.   A Mad Tea-Party\\n CHAPTER VIII.  The Queen’s Croquet-Ground\\n CHAPTER IX.    The Mock Turtle’s Story\\n CHAPTER X.     The Lobster Quadrille\\n CHAPTER XI.    Who Stole the Tarts?\\n CHAPTER XII.   Alice’s Evidence\\n\\n\\n\\n\\nCHAPTER I.\\nDown the Rabbit-Hole\\n\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into\\nthe book her sister was reading, but it had no pictures or\\nconversations in it, “and what is the use of a book,” thought Alice\\n“without pictures or conversations?”\\n\\nSo she was considering in her own mind (as well as she could, for the\\nhot day made her feel very '"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "pacific-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to lower case\n",
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "found-lawyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing numbers, non-alphabetic characters\n",
    "for symb in '0123456789()[];,:?!.@#$%^&*\"—”“_':\n",
    "    text = text.replace(symb, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "documented-formula",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing '\\n' symbol\n",
    "text = text.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "experienced-watershed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing ’s symbols\n",
    "text = text.replace('’s', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "timely-creation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alice adventures in wonderland  by lewis carroll  the millennium fulcrum edition   contents   chapter i     down the rabbit-hole  chapter ii    the pool of tears  chapter iii   a caucus-race and a long tale  chapter iv    the rabbit sends in a little bill  chapter v     advice from a caterpillar  chapter vi    pig and pepper  chapter vii   a mad tea-party  chapter viii  the queen croquet-ground  chapter ix    the mock turtle story  chapter x     the lobster quadrille  chapter xi    who stole the tarts  chapter xii   alice evidence     chapter i down the rabbit-hole   alice was beginning to get very tired of sitting by her sister on the bank and of having nothing to do once or twice she had peeped into the book her sister was reading but it had no pictures or conversations in it and what is the use of a book thought alice without pictures or conversations  so she was considering in her own mind as well as she could for the hot day made her feel very sleepy and stupid whether the pleasur'"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "sunrise-arrival",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = re.findall(r\"\\w+\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "strategic-locator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alice',\n",
       " 'adventures',\n",
       " 'in',\n",
       " 'wonderland',\n",
       " 'by',\n",
       " 'lewis',\n",
       " 'carroll',\n",
       " 'the',\n",
       " 'millennium',\n",
       " 'fulcrum',\n",
       " 'edition',\n",
       " 'contents',\n",
       " 'chapter',\n",
       " 'i',\n",
       " 'down',\n",
       " 'the',\n",
       " 'rabbit',\n",
       " 'hole',\n",
       " 'chapter',\n",
       " 'ii',\n",
       " 'the',\n",
       " 'pool',\n",
       " 'of',\n",
       " 'tears',\n",
       " 'chapter']"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "intensive-mining",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "precise-booking",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters_nums = ['i', 'ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'x', 'xi', 'xii']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "filled-annotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stop words and lemmatization\n",
    "proceed_words = []\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "for word in words:\n",
    "    if word not in stop_words + chapters_nums:\n",
    "        proceed_words.append(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "fitted-jersey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proceed verbs to infinitive form\n",
    "for i in range(len(proceed_words)):\n",
    "    proceed_words[i] = lemmatizer.lemmatize(proceed_words[i], 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "reflected-manor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27066"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial number of words\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "internal-intervention",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12262"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of words after lemmatization and removing stop words\n",
    "len(proceed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "treated-pantyhose",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alice',\n",
       " 'adventure',\n",
       " 'wonderland',\n",
       " 'lewis',\n",
       " 'carroll',\n",
       " 'millennium',\n",
       " 'fulcrum',\n",
       " 'edition',\n",
       " 'content',\n",
       " 'chapter',\n",
       " 'rabbit',\n",
       " 'hole',\n",
       " 'chapter',\n",
       " 'pool',\n",
       " 'tear',\n",
       " 'chapter',\n",
       " 'caucus',\n",
       " 'race',\n",
       " 'long',\n",
       " 'tale',\n",
       " 'chapter',\n",
       " 'rabbit',\n",
       " 'send',\n",
       " 'little',\n",
       " 'bill']"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proceed_words[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-membrane",
   "metadata": {},
   "source": [
    "### Split text into the chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "orange-collective",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of chapters in the book\n",
    "chapters_num = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "thrown-relation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text to the chapters\n",
    "chapters = [[] for _ in range(chapters_num)]\n",
    "\n",
    "i_word = 0\n",
    "i_chapter = 0\n",
    "\n",
    "for i in range(len(proceed_words)):\n",
    "    if proceed_words[i] == 'chapter':\n",
    "        i_chapter += 1\n",
    "        \n",
    "        part = proceed_words[i_word+1:i]\n",
    "        i_word = i\n",
    "        if i_chapter >= 13:\n",
    "            chapters[i_chapter-14] = part\n",
    "            \n",
    "chapters[11] = proceed_words[i_word+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "occasional-stake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing 'alice'\n",
    "for chapter in chapters:\n",
    "    while 'alice' in chapter:\n",
    "        ind = chapter.index('alice')\n",
    "        chapter.pop(ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-nurse",
   "metadata": {},
   "source": [
    "### Top 10 words from each chapter  (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "worse-queen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a TF-IDF vectorizer object\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "entitled-panic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join words for every chapter into one text for every chapter\n",
    "chapters_texts = [' '.join(words) for words in chapters] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "cultural-europe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(chapters_texts).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "intended-father",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.vocabulary_\n",
    "reverse_vocab = {v:k for k,v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "applicable-gauge",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = X.argsort(axis=1)\n",
    "tfidf_max10 = idx[:,-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "configured-necessity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe TF-IDF\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "df_tfidf = pd.DataFrame(X, columns = feature_names)\n",
    "df_tfidf['top10'] = [[reverse_vocab.get(item) for item in row] for row in tfidf_max10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "designed-affairs",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abide</th>\n",
       "      <th>able</th>\n",
       "      <th>absence</th>\n",
       "      <th>absurd</th>\n",
       "      <th>acceptance</th>\n",
       "      <th>accident</th>\n",
       "      <th>accidentally</th>\n",
       "      <th>account</th>\n",
       "      <th>accusation</th>\n",
       "      <th>accustom</th>\n",
       "      <th>...</th>\n",
       "      <th>yetit</th>\n",
       "      <th>yetoh</th>\n",
       "      <th>youall</th>\n",
       "      <th>youare</th>\n",
       "      <th>youcome</th>\n",
       "      <th>young</th>\n",
       "      <th>youth</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zigzag</th>\n",
       "      <th>top10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030767</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>[key, rabbit, door, get, bat, go, little, say,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>[cry, swim, dear, think, cat, little, pool, sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025644</td>\n",
       "      <td>0.029859</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029859</td>\n",
       "      <td>0.020332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>[bird, thimble, dry, know, lory, race, prize, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025106</td>\n",
       "      <td>0.025106</td>\n",
       "      <td>0.025106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>[get, go, fan, grow, say, puppy, rabbit, windo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015832</td>\n",
       "      <td>0.139503</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.02325</td>\n",
       "      <td>[little, father, think, size, egg, youth, serp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.021287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018282</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>[wow, sneeze, grin, duchess, go, mad, baby, ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.013550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>[draw, time, go, twinkle, tea, march, hare, do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.016091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>[five, cat, soldier, look, gardener, go, king,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026798</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>[school, think, go, queen, moral, duchess, gry...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.013343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>[join, soooop, soup, beautiful, lobster, dance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022354</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>[juror, officer, queen, jury, witness, dormous...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.05102</td>\n",
       "      <td>0.02551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>[rabbit, would, slate, sister, queen, write, d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 1912 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       abide      able   absence    absurd  acceptance  accident  \\\n",
       "0   0.000000  0.000000  0.000000  0.000000    0.000000   0.00000   \n",
       "1   0.000000  0.030907  0.000000  0.000000    0.000000   0.00000   \n",
       "2   0.000000  0.000000  0.000000  0.025644    0.029859   0.00000   \n",
       "3   0.000000  0.000000  0.000000  0.000000    0.000000   0.00000   \n",
       "4   0.000000  0.000000  0.000000  0.000000    0.000000   0.00000   \n",
       "5   0.021287  0.000000  0.000000  0.018282    0.000000   0.00000   \n",
       "6   0.000000  0.000000  0.000000  0.000000    0.000000   0.00000   \n",
       "7   0.000000  0.000000  0.000000  0.000000    0.000000   0.00000   \n",
       "8   0.000000  0.000000  0.019678  0.000000    0.000000   0.00000   \n",
       "9   0.000000  0.000000  0.000000  0.000000    0.000000   0.00000   \n",
       "10  0.000000  0.000000  0.000000  0.000000    0.000000   0.00000   \n",
       "11  0.000000  0.000000  0.000000  0.000000    0.000000   0.05102   \n",
       "\n",
       "    accidentally   account  accusation  accustom  ...     yetit     yetoh  \\\n",
       "0        0.00000  0.000000    0.000000  0.000000  ...  0.000000  0.000000   \n",
       "1        0.00000  0.000000    0.000000  0.000000  ...  0.000000  0.000000   \n",
       "2        0.00000  0.000000    0.000000  0.029859  ...  0.000000  0.000000   \n",
       "3        0.00000  0.000000    0.000000  0.000000  ...  0.025106  0.025106   \n",
       "4        0.00000  0.000000    0.000000  0.000000  ...  0.000000  0.000000   \n",
       "5        0.00000  0.000000    0.000000  0.000000  ...  0.000000  0.000000   \n",
       "6        0.00000  0.013550    0.000000  0.000000  ...  0.000000  0.000000   \n",
       "7        0.00000  0.016091    0.000000  0.000000  ...  0.000000  0.000000   \n",
       "8        0.00000  0.000000    0.000000  0.000000  ...  0.000000  0.000000   \n",
       "9        0.00000  0.013343    0.000000  0.000000  ...  0.000000  0.000000   \n",
       "10       0.00000  0.000000    0.022354  0.000000  ...  0.000000  0.000000   \n",
       "11       0.02551  0.000000    0.000000  0.000000  ...  0.000000  0.000000   \n",
       "\n",
       "      youall    youare   youcome     young     youth   zealand   zigzag  \\\n",
       "0   0.000000  0.000000  0.000000  0.000000  0.000000  0.030767  0.00000   \n",
       "1   0.000000  0.030907  0.000000  0.000000  0.000000  0.000000  0.00000   \n",
       "2   0.000000  0.000000  0.029859  0.020332  0.000000  0.000000  0.00000   \n",
       "3   0.025106  0.000000  0.000000  0.000000  0.000000  0.000000  0.00000   \n",
       "4   0.000000  0.000000  0.000000  0.015832  0.139503  0.000000  0.02325   \n",
       "5   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.00000   \n",
       "6   0.000000  0.000000  0.000000  0.012162  0.000000  0.000000  0.00000   \n",
       "7   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.00000   \n",
       "8   0.000000  0.000000  0.000000  0.026798  0.000000  0.000000  0.00000   \n",
       "9   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.00000   \n",
       "10  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.00000   \n",
       "11  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.00000   \n",
       "\n",
       "                                                top10  \n",
       "0   [key, rabbit, door, get, bat, go, little, say,...  \n",
       "1   [cry, swim, dear, think, cat, little, pool, sa...  \n",
       "2   [bird, thimble, dry, know, lory, race, prize, ...  \n",
       "3   [get, go, fan, grow, say, puppy, rabbit, windo...  \n",
       "4   [little, father, think, size, egg, youth, serp...  \n",
       "5   [wow, sneeze, grin, duchess, go, mad, baby, ca...  \n",
       "6   [draw, time, go, twinkle, tea, march, hare, do...  \n",
       "7   [five, cat, soldier, look, gardener, go, king,...  \n",
       "8   [school, think, go, queen, moral, duchess, gry...  \n",
       "9   [join, soooop, soup, beautiful, lobster, dance...  \n",
       "10  [juror, officer, queen, jury, witness, dormous...  \n",
       "11  [rabbit, would, slate, sister, queen, write, d...  \n",
       "\n",
       "[12 rows x 1912 columns]"
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "finite-mistake",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-10 words in chapter # 1: key, rabbit, door, get, bat, go, little, say, eat, think\n",
      "Top-10 words in chapter # 2: cry, swim, dear, think, cat, little, pool, say, go, mouse\n",
      "Top-10 words in chapter # 3: bird, thimble, dry, know, lory, race, prize, dodo, mouse, say\n",
      "Top-10 words in chapter # 4: get, go, fan, grow, say, puppy, rabbit, window, little, bill\n",
      "Top-10 words in chapter # 5: little, father, think, size, egg, youth, serpent, pigeon, caterpillar, say\n",
      "Top-10 words in chapter # 6: wow, sneeze, grin, duchess, go, mad, baby, cat, footman, say\n",
      "Top-10 words in chapter # 7: draw, time, go, twinkle, tea, march, hare, dormouse, say, hatter\n",
      "Top-10 words in chapter # 8: five, cat, soldier, look, gardener, go, king, hedgehog, say, queen\n",
      "Top-10 words in chapter # 9: school, think, go, queen, moral, duchess, gryphon, mock, turtle, say\n",
      "Top-10 words in chapter # 10: join, soooop, soup, beautiful, lobster, dance, say, gryphon, mock, turtle\n",
      "Top-10 words in chapter # 11: juror, officer, queen, jury, witness, dormouse, court, say, hatter, king\n",
      "Top-10 words in chapter # 12: rabbit, would, slate, sister, queen, write, dream, jury, king, say\n"
     ]
    }
   ],
   "source": [
    "# Get top-10 words for every chapter\n",
    "for i in range(chapters_num):\n",
    "    print(f'Top-10 words in chapter # {i+1}: {\", \".join(df_tfidf[\"top10\"][i])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-blond",
   "metadata": {},
   "source": [
    "### Top 10 most used verbs in sentences with Alice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-vessel",
   "metadata": {},
   "source": [
    "To find top 10 most used verbs the idea is following: \n",
    "\n",
    "1. Find all verbs after 'alice'\n",
    "2. Filter verbs from these words\n",
    "3. Find most used words from filtered verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "printable-consistency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stop words and lemmatization\n",
    "proceed_words = []\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "for word in words:\n",
    "    if word not in stop_words + chapters_nums:\n",
    "        proceed_words.append(lemmatizer.lemmatize(word, 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "id": "extreme-replication",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_after_alice = []\n",
    "items_verbs = []\n",
    "\n",
    "for i in range(len(tagged_tokens)):\n",
    "    if tagged_tokens[i][0] == 'alice':\n",
    "        if not tagged_tokens[i+1][1] in ('NN', 'NNS', 'PRP', 'JJ', 'WP'):\n",
    "            items_after_alice.append([tagged_tokens[i+1], tagged_tokens[i+2]])\n",
    "            if tagged_tokens[i+1][1] in ('VBD', 'VBP', 'VB'):\n",
    "                if tagged_tokens[i+2][1] not in ('VBD', 'VBP', 'VBN', 'VBG'):\n",
    "                    items_verbs.append([tagged_tokens[i+1]])\n",
    "                else:\n",
    "                    items_verbs.append([tagged_tokens[i+1], tagged_tokens[i+2]])\n",
    "            elif tagged_tokens[i+2][1] in ('VBD', 'VBP', 'VBN', 'VBG'):\n",
    "                items_verbs.append([tagged_tokens[i+2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "id": "conservative-transition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324"
      ]
     },
     "execution_count": 698,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(items_after_alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "id": "approved-token",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('was', 'VBD'), ('beginning', 'VBG')],\n",
       " [('think', 'VB')],\n",
       " [('started', 'VBD')],\n",
       " [('had', 'VBD')],\n",
       " [('had', 'VBD')],\n",
       " [('had', 'VBD')],\n",
       " [('began', 'VBD')],\n",
       " [('began', 'VBD')],\n",
       " [('was', 'VBD')],\n",
       " [('had', 'VBD'), ('been', 'VBN')],\n",
       " [('thought', 'VBD')],\n",
       " [('opened', 'VBD')],\n",
       " [('had', 'VBD'), ('begun', 'VBN')],\n",
       " [('was', 'VBD')],\n",
       " [('had', 'VBD'), ('got', 'VBN')],\n",
       " [('felt', 'VBD')],\n",
       " [('took', 'VBD')],\n",
       " [('had', 'VBD'), ('been', 'VBN')],\n",
       " [('thought', 'VBD')],\n",
       " [('had', 'VBD')],\n",
       " [('afraid', 'VBP')],\n",
       " [('went', 'VBD')],\n",
       " [('went', 'VBD')],\n",
       " [('thought', 'VBD')],\n",
       " [('led', 'VBD')]]"
      ]
     },
     "execution_count": 713,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_verbs[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "id": "disciplinary-sound",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat data into phrases with verbs\n",
    "phrases = []\n",
    "for item_pair in items_verbs:\n",
    "    if len(item_pair) == 2:\n",
    "        phrases.append([item_pair[0][0], item_pair[1][0]])\n",
    "    else:\n",
    "        phrases.append([item_pair[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "id": "mexican-landing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['was', 'beginning'],\n",
       " ['think'],\n",
       " ['started'],\n",
       " ['had'],\n",
       " ['had'],\n",
       " ['had'],\n",
       " ['began'],\n",
       " ['began'],\n",
       " ['was'],\n",
       " ['had', 'been'],\n",
       " ['thought'],\n",
       " ['opened'],\n",
       " ['had', 'begun'],\n",
       " ['was'],\n",
       " ['had', 'got'],\n",
       " ['felt'],\n",
       " ['took'],\n",
       " ['had', 'been'],\n",
       " ['thought'],\n",
       " ['had'],\n",
       " ['afraid'],\n",
       " ['went'],\n",
       " ['went'],\n",
       " ['thought'],\n",
       " ['led']]"
      ]
     },
     "execution_count": 714,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "id": "coordinated-details",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove useless words and \n",
    "proceed_phrases = []\n",
    "\n",
    "for phrase in phrases:\n",
    "    new_phrase = []\n",
    "    for word in phrase:\n",
    "        if word not in stop_words:\n",
    "            new_phrase.append(word)\n",
    "    if new_phrase:\n",
    "        proceed_phrases.append(new_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "id": "starting-realtor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['beginning'],\n",
       " ['think'],\n",
       " ['started'],\n",
       " ['began'],\n",
       " ['began'],\n",
       " ['thought'],\n",
       " ['opened'],\n",
       " ['begun'],\n",
       " ['got'],\n",
       " ['felt'],\n",
       " ['took'],\n",
       " ['thought'],\n",
       " ['afraid'],\n",
       " ['went'],\n",
       " ['went'],\n",
       " ['thought'],\n",
       " ['led'],\n",
       " ['kept'],\n",
       " ['thought'],\n",
       " ['sighing'],\n",
       " ['replied'],\n",
       " ['began'],\n",
       " ['said'],\n",
       " ['went'],\n",
       " ['knew']]"
      ]
     },
     "execution_count": 717,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proceed_phrases[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "id": "concrete-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get words into infinitive form\n",
    "verbs = []\n",
    "\n",
    "for i in range(len(proceed_phrases)):\n",
    "    word = proceed_phrases[i][0]\n",
    "    verbs.append(lemmatizer.lemmatize(word, 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "id": "curious-entrance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['begin',\n",
       " 'think',\n",
       " 'start',\n",
       " 'begin',\n",
       " 'begin',\n",
       " 'think',\n",
       " 'open',\n",
       " 'begin',\n",
       " 'get',\n",
       " 'felt',\n",
       " 'take',\n",
       " 'think',\n",
       " 'afraid',\n",
       " 'go',\n",
       " 'go',\n",
       " 'think',\n",
       " 'lead',\n",
       " 'keep',\n",
       " 'think',\n",
       " 'sigh',\n",
       " 'reply',\n",
       " 'begin',\n",
       " 'say',\n",
       " 'go',\n",
       " 'know']"
      ]
     },
     "execution_count": 723,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbs[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "id": "worthy-canadian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 728,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "id": "alternate-culture",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = nltk.FreqDist(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "id": "located-worse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'say': 13, 'think': 12, 'reply': 11, 'begin': 10, 'look': 10, 'felt': 5, 'go': 5, 'hear': 3, 'wait': 3, 'get': 2, ...})"
      ]
     },
     "execution_count": 727,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-recycling",
   "metadata": {},
   "source": [
    "Top-10 verbs with Alice are: say, think, reply, begin, look, felt, go, hear, wait, get."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
